\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage[warn]{mathtext}
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы
\usepackage{indentfirst}
\usepackage{csquotes}
% \usepackage[bibstyle=gost-numeric, sorting=none]{biblatex}
% \addbibresource{biblio.bib}

\linespread{1.3}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{hyphenat} 

\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}

% page settings
\usepackage[
    left=1.5cm,
    right=1.5cm,
    top=1.5cm,
    bottom=1.5cm,
    bindingoffset=0cm
]{geometry}

\usepackage{graphicx, hyperref, xcolor}
\hypersetup{
    colorlinks=true,
    linkcolor=teal,
    filecolor=magenta, 
    urlcolor=blue,
    citecolor=olive,
    pdftitle={GD},
    % pdfpagemode=FullScreen,
    linktoc=all
    }

\usepackage{wrapfig,caption}

% figures
\usepackage{caption}
\usepackage{subcaption}
\usepackage{floatrow}
\floatsetup{heightadjust=object}

% math
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools,esint,eucal}


\def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle#1$}}
{\mbox{\boldmath$\textstyle#1$}} {\mbox{\boldmath$\scriptstyle#1$}} {\mbox{\boldmath$\scriptscriptstyle#1$}}}

\title{Эффективное сжатие мультимодальных видео-моделей на примере ViT с использованием квантизации.}


\author{Neychev R. G.\\
	Lomonosov Moscow State University\\
	Faculty of Computational Mathematics and Cybernetics\\
	Moscow \\
	\texttt{hippo@cs.cranberry-lemon.edu} \\
	\And
	Аkopova Е. N.\\
	Lomonosov Moscow State University\\
	Faculty of Computational Mathematics and Cybernetics\\
	Moscow\\
	\texttt{s0220010@gse.cs.msu.ru} \\
}
\date{}

\renewcommand{\shorttitle}{Эффективное обучение.}

\hypersetup{
pdftitle={My first paper Akopova},
pdfkeywords={Human pose estimation, visual-language understanding},
}

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\keywords{human pose estimation \and visual-language understanding }

\newpage
\section{Введение}

Vision Transformer (ViT) стали ключевым компонентом современных мультимодальных видео-моделей, однако их вычислительная сложность ограничивает применение в реальных сценариях. Эффективное сжатие ViT критически важно для развертывания видео-моделей на мобильных устройствах и в условиях ограниченных ресурсов. Квантизация представляет наиболее перспективный подход к уменьшению размера и ускорению работы ViT-архитектур в контексте обработки видео.

В работах P2-ViT предложена эффективная схема пост-тренировочной квантизации с использованием степеней двойки, позволяющая заменить операции умножения битовыми сдвигами. Исследования в области Model Quantization for Vision Transformers систематизируют различные стратегии квантизации внимания и MLP-блоков. Методы Quantization-Aware Training адаптируют модель к пониженной точности во время обучения, а схемы смешанной точности позволяют дифференцированно подходить к квантизации различных слоев трансформера.

Стандартные методы квантизации не учитывают временную природу видео-данных и особенности работы ViT в составе мультимодальных систем. Наблюдается значительная деградация качества при агрессивной квантизации механизмов внимания, критически важных для анализа видео-последовательностей. Отсутствуют специализированные подходы к квантизации ViT-энкодеров, работающих с видео-фреймами, где необходимо сохранять временную согласованность представлений.

В работе разрабатывается специализированная методика квантизации ViT для видео-приложений, учитывающая временные зависимости между кадрами. Предлагается адаптивная стратегия выбора битности для различных компонентов ViT на основе анализа их вклада в итоговое качество мультимодальной задачи. Создается механизм сохранения пространственно-временных особенностей видео при переходе к низкоразрядным представлениям.

Ожидается достижение коэффициента сжатия ViT-компонента в 3.5-4 раза при сохранении 96-98\% исходной точности на видео-задачах. Практическим результатом станет оптимизированная версия ViT-энкодера для видео-приложений с детализированными метриками эффективности. Научная новизна заключается в разработке временно-осознанных методов квантизации для ViT и создании методики оценки влияния квантизации на качество работы в мультимодальном контексте.

\section{Обзор литературы}

В работе \textbf{«P2-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer»} предложена инновационная схема пост-тренировочной квантизации для Vision Transformer, основанная на степенях двойки. Ключевая идея заключается в использовании power-of-two квантовых уровней, что позволяет заменить ресурсоёмкие операции умножения на эффективные битовые сдвиги при инференсе. Авторы разработали полностью квантизованную архитектуру ViT, включая механизмы внимания и многослойные перцептроны, что обеспечивает значительное ускорение вычислений на специализированных аппаратных ускорителях. Особый интерес представляет предложенный метод калибровки, который минимизирует потерю информации при квантизации за счёт учёта распределения активаций в различных слоях трансформера.

Исследование \textbf{«Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation»} фокусируется на проблеме квантизации мультимодальных больших языковых моделей. Авторы предлагают метод Quantization-Aware Scale Learning (QASL), который интегрирует обучение параметров масштабирования непосредственно в процесс адаптации модели. Этот подход позволяет эффективно балансировать между точностью и эффективностью при работе с разнородными модальностями (текст, изображение, видео). Важным вкладом работы является демонстрация того, что учёт особенностей мультимодального выравнивания в процессе квантизации позволяет сохранить семантическую согласованность между различными типами данных.

В обзорной статье \textbf{«Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey»} представлена систематизация современных методов квантизации и аппаратного ускорения для Vision Transformer. Авторы детально анализируют различные схемы квантизации (post-training quantization, quantization-aware training), рассматривают особенности квантизации ключевых компонентов ViT (включая multi-head attention и feed-forward сети), а также проводят сравнительный анализ эффективности различных подходов на различных аппаратных платформах. Особую ценность представляет классификация методов по степени сжатия, точности и требованиям к вычислительным ресурсам, что позволяет выбирать оптимальную стратегию квантизации для конкретных прикладных задач.

Рассмотренные работы демонстрируют эволюцию методов квантизации от общих подходов к специализированным решениям для трансформерных архитектур. Наблюдается переход от стандартных схем квантизации к методам, учитывающим специфику архитектуры Vision Transformer и мультимодальных моделей. Современные исследования фокусируются на разработке аппаратно-ориентированных решений (P\^2-ViT), адаптивных методов для сложных архитектур (QASL) и систематизации накопленных знаний (Comprehensive Survey). Перспективным направлением является создание универсальных фреймворков квантизации, способных автоматически адаптироваться к особенностям различных архитектур и аппаратных платформ, обеспечивая оптимальный баланс между точностью, скоростью работы и энергоэффективностью.

\section{Постановка задачи квантизации ViT для классификации изображений}

Пусть задана модель классификации изображений $f: \mathcal{X} \rightarrow \mathcal{Y}$, где $\mathcal{X}$ --- пространство изображений, $\mathcal{Y}$ --- пространство меток классов. Модель $f$ представляет собой Vision Transformer (ViT), параметризованную весами $\mathbf{W} = \{W_1, W_2, \dots, W_N\}$, где $N$ --- общее количество параметров.

Модель обучается на датасете $\mathcal{D}_{\text{train}} = \{(x_i, y_i)\}_{i=1}^M$ и оценивается на датасете $\mathcal{D}_{\text{val}} = \{(x_j, y_j)\}_{j=1}^K$ с помощью метрики точности:

\[
\text{Accuracy}(f) = \frac{1}{K} \sum_{j=1}^K \mathbb{I}[f(x_j) = y_j].
\]

Задача квантизации состоит в нахождении отображения $Q: \mathbb{R} \rightarrow \mathbb{Z}$, преобразующего веса модели из формата с плавающей точкой (FP32/FP16) в целочисленное представление (INT8) с минимальной потерей точности. Формально, требуется найти квантизованную модель $f_Q$ с весами $\mathbf{W}_Q = Q(\mathbf{W})$ такую, что:

\[
\text{Accuracy}(f_Q) \approx \text{Accuracy}(f),
\]
при этом достигается значительное сокращение памяти и ускорение вывода.

Рассматривается схема W8A8 (8-битные веса и активации), реализуемая через замену линейных слоёв $\text{Linear}(\mathbf{W}, \mathbf{b})$ на квантизованные версии $\text{W8A8Linear}(\mathbf{W}_Q, \mathbf{b}_Q)$, где:

\[
\mathbf{W}_Q = \text{quantize\_weight}(\mathbf{W}), \quad \mathbf{b}_Q = \mathbf{b},
\]
\[
\mathbf{x}_Q = \text{quantize\_activation}(\mathbf{x}),
\]
\[
\mathbf{y} = \text{dequantize}(\mathbf{W}_Q \mathbf{x}_Q + \mathbf{b}_Q).
\]

Качество квантизации оценивается по:
\begin{itemize}
    \item Относительному падению точности: $\Delta_{\text{acc}} = \dfrac{\text{Accuracy}(f) - \text{Accuracy}(f_Q)}{\text{Accuracy}(f)}$
    \item Коэффициенту сжатия: $r = \dfrac{\text{Memory}(f)}{\text{Memory}(f_Q)}$
    \item Ускорению инференса: $s = \dfrac{\text{Latency}(f)}{\text{Latency}(f_Q)}$
\end{itemize}

Требуется исследовать влияние различных стратегий квантизации (per-channel, per-token) на разные компоненты ViT (attention, MLP) и определить оптимальный баланс между эффективностью и точностью для заданного семейства моделей $\mathcal{F} = \{f_{\theta}\}_{\theta \in \Theta}$.


\section{Экспериментальное исследование}

Для проведения экспериментов был выбран датасет CIFAR-10, содержащий 60 000 цветных изображений размером 32×32 пикселя, разделенных на 10 классов: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. Датасет разделен на обучающую (50 000 изображений) и тестовую (10 000 изображений) выборки.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{cifar10_samples.png}
\caption{Примеры изображений из датасета CIFAR-10}
\label{fig:cifar10_samples}
\end{figure}

Для адаптации изображений к входным требованиям модели ViT применялась процедура предобработки, включающая этап ресайзинга (размер изображения был изменен до 224×224 пикселей с использованием бикубической интерполяции), нормализации (значения пикселей приведены к диапазону [0,1] с последующей нормализацией по статистике ImageNet) и аугментации (применены случайные горизонтальные отражения и небольшие вращения для увеличения разнообразия данных).

Преобразования описываются следующей формулой:
\[
x_{\text{norm}} = \frac{x_{\text{resized}} - \mu}{\sigma}
\]
где $\mu = [0.485, 0.456, 0.406]$, $\sigma = [0.229, 0.224, 0.225]$ - средние и стандартные отклонения для каждого канала.

В качестве базовой модели использовалась Vision Transformer "google/vit-base-patch16-224" со следующими характеристиками
размер патча - 16×16 пикселей, количество слоев - 12, размер скрытого состояния: 768, количество голов внимания - 12, общее количество параметров: 86 млн.

Для адаптации модели к задаче классификации CIFAR-10 была заменена головка классификации с 1000 на 10 выходных нейронов. Процесс дообучения проводился со следующими гиперпараметрами:
\begin{table}[h]
\centering
\caption{Гиперпараметры дообучения ViT}
\begin{tabular}{ll}
\toprule
\textbf{Параметр} & \textbf{Значение} \\
\midrule
Оптимизатор & AdamW \\
Learning rate & $2 \times 10^{-5}$ \\
Размер батча & 32 \\
Количество эпох & 10 \\
Функция потерь & CrossEntropyLoss \\
Планировщик & LinearLR с warmup \\
\bottomrule
\end{tabular}
\label{tab:hyperparams}
\end{table}


Квантизация проводилась с использованием метода W8A8 (8-битные веса и активации) с применением пер-ченнельной квантизации для весов и пер-токенной квантизации для активаций. Процесс включал следующие этапы:

\begin{enumerate}
    \item \textbf{Калибровка}: Прогон небольшого подмножества данных (512 изображений) для сбора статистики активаций
    \item \textbf{Квантизация весов}: Преобразование параметров линейных слоев в 8-битный формат
    \item \textbf{Квантизация активаций}: Внедрение квантизационных операторов после каждого слоя внимания и MLP
\end{enumerate}

Математически процесс квантизации описывается формулами:
$$
W_q = \text{round}\left(\frac{W}{\text{scale}_w}\right) \cdot \text{scale}_w, \quad
A_q = \text{round}\left(\frac{A}{\text{scale}_a}\right) \cdot \text{scale}_a
$$

После дообучения и квантизации были получены следующие результаты:

\begin{table}[h]
\centering
\caption{Сравнение характеристик моделей до и после квантизации}
\begin{tabular}{|l|c|c|c|c|}
\hline
Модель & Точность (\%) & Размер (МБ) & Время инференса (мс) & Память (МБ) \\
\hline
ViT-base (FP32) & 98.2 & 327 & 15.3 & 1250 \\
ViT-base (FP16) & 98.1 & 164 & 8.7 & 680 \\
ViT-base (W8A8) & 97.8 & 82 & 5.2 & 350 \\
\hline
\end{tabular}
\label{tab:results}
\end{table}

Анализ результатов показывает, что произошла незначительная потеря точности. А именно: падение точности составило всего 0.4\% при переходе к 8-битной квантизации. Также было получено значительное сжатие модели: размер уменьшился в 4 раза по сравнению с FP32 версией. Наблюдалось ускорение инференса и экономия памяти: время обработки сократилось в 3 раза, а потребление видеопамяти уменьшилось в 3.6 раза.

\bibliographystyle{unsrtnat}
\bibliography{references}


\end{document}